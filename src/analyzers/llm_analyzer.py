"""LLM-based article analyzer with multi-provider support."""

import asyncio
import json
from typing import List, Union
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage
from src.models.article import Article
from src.models.analysis import AnalysisResult, CategoryType
from src.config import settings


def create_llm() -> BaseChatModel:
    """Create LLM instance based on configured provider.

    Returns:
        Configured LLM instance

    Raises:
        ValueError: If provider is not supported or API key is missing
    """
    provider = settings.llm_provider

    if provider == "anthropic":
        if not settings.anthropic_api_key:
            raise ValueError("ANTHROPIC_API_KEY is required for Anthropic provider")

        from langchain_anthropic import ChatAnthropic

        return ChatAnthropic(
            model=settings.llm_model,
            api_key=settings.anthropic_api_key,
            temperature=settings.llm_temperature,
            max_tokens=settings.llm_max_tokens,
        )

    elif provider == "openai":
        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY is required for OpenAI provider")

        from langchain_openai import ChatOpenAI

        return ChatOpenAI(
            model=settings.llm_model,
            api_key=settings.openai_api_key,
            temperature=settings.llm_temperature,
            max_tokens=settings.llm_max_tokens,
        )

    elif provider == "google":
        if not settings.google_api_key:
            raise ValueError("GOOGLE_API_KEY is required for Google provider")

        from langchain_google_genai import ChatGoogleGenerativeAI

        return ChatGoogleGenerativeAI(
            model=settings.llm_model,
            google_api_key=settings.google_api_key,
            temperature=settings.llm_temperature,
            max_output_tokens=settings.llm_max_tokens,
        )

    else:
        raise ValueError(
            f"Unsupported LLM provider: {provider}. "
            f"Supported providers: anthropic, openai, google"
        )


class LLMAnalyzer:
    """Analyzes articles using configurable LLM."""

    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ AI æŠ€è¡“å°ˆå®¶ï¼Œè² è²¬åˆ†æž AI æ–°èžæ–‡ç« å’Œç ”ç©¶è«–æ–‡ã€‚

ä½ çš„ä»»å‹™æ˜¯ç‚ºæ¯ç¯‡æ–‡ç« æä¾›ï¼š
1. **ä¸­æ–‡æ¨™é¡Œ** (title_cn): å°‡æ–‡ç« æ¨™é¡Œç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼Œä¿æŒå°ˆæ¥­è¡“èªžæº–ç¢º
2. **æ‘˜è¦** (summary): 100å­—ä»¥å…§çš„ç¹é«”ä¸­æ–‡ç²¾ç…‰æ‘˜è¦ï¼Œçªå‡ºæ ¸å¿ƒå…§å®¹
3. **åˆ†é¡ž** (category): é¸æ“‡æœ€åˆé©çš„é¡žåˆ¥ï¼š
   - "Breaking News": é‡å¤§æ–°èžç™¼å¸ƒã€ç”¢å“ç™¼å¸ƒ
   - "Research": å­¸è¡“ç ”ç©¶ã€è«–æ–‡
   - "Tools/Products": æ–°å·¥å…·ã€ç”¢å“ã€é–‹æºé …ç›®
   - "Business": å•†æ¥­å‹•æ…‹ã€èžè³‡ã€ä½µè³¼
   - "Tutorial": æ•™ç¨‹ã€æŠ€è¡“æ–‡ç« ã€å¯¦è¸æŒ‡å—
4. **é‡è¦æ€§è©•åˆ†** (importance_score): 0-10åˆ†ï¼Œè©•ä¼°æ–‡ç« çš„é‡è¦æ€§å’Œå½±éŸ¿åŠ›
   - 9-10: è¡Œæ¥­çªç ´æ€§é€²å±•
   - 7-8: é‡è¦æ–°èžæˆ–ç ”ç©¶
   - 5-6: æœ‰åƒ¹å€¼çš„æ›´æ–°
   - 0-4: ä¸€èˆ¬æ€§å…§å®¹
5. **é—œéµæ´žå¯Ÿ** (insight): 50å­—ä»¥å…§çš„ç¹é«”ä¸­æ–‡é—œéµæ´žå¯Ÿï¼Œèªªæ˜Žç‚ºä»€éº¼é‡è¦ã€æœ‰ä»€éº¼å½±éŸ¿

è«‹ä»¥ JSON æ ¼å¼è¿”å›žçµæžœï¼ˆç¢ºä¿æ‰€æœ‰ä¸­æ–‡å…§å®¹ç‚ºç¹é«”ä¸­æ–‡ï¼‰ï¼š
{
  "title_cn": "æ–‡ç« ä¸­æ–‡æ¨™é¡Œ...",
  "summary": "æ–‡ç« æ‘˜è¦...",
  "category": "Breaking News",
  "importance_score": 8,
  "insight": "é—œéµæ´žå¯Ÿ..."
}"""

    ANALYSIS_PROMPT_TEMPLATE = """è«‹åˆ†æžä»¥ä¸‹æ–‡ç« ï¼š

**æ¨™é¡Œ**: {title}

**ä¾†æº**: {source}

**å…§å®¹**:
{content}

è«‹æä¾›åˆ†æžçµæžœï¼ˆJSON æ ¼å¼ï¼‰ï¼š"""

    def __init__(self):
        """Initialize LLM analyzer with configured provider."""
        self.llm = create_llm()
        print(f"ðŸ“Š LLM Provider: {settings.llm_provider}")
        print(f"ðŸ“Š LLM Model: {settings.llm_model}")

    async def analyze_batch(self, articles: List[Article]) -> List[AnalysisResult]:
        """Analyze multiple articles concurrently.

        Args:
            articles: List of articles to analyze

        Returns:
            List of AnalysisResult objects
        """
        # Process in batches of 5 to avoid rate limits
        batch_size = 5
        results = []

        for i in range(0, len(articles), batch_size):
            batch = articles[i : i + batch_size]
            batch_results = await asyncio.gather(
                *[self.analyze(article) for article in batch], return_exceptions=True
            )

            for result in batch_results:
                if isinstance(result, AnalysisResult):
                    results.append(result)
                elif isinstance(result, Exception):
                    print(f"Error analyzing article: {result}")

            # Small delay between batches
            if i + batch_size < len(articles):
                await asyncio.sleep(1)

        return results

    async def analyze(self, article: Article) -> AnalysisResult:
        """Analyze a single article.

        Args:
            article: Article to analyze

        Returns:
            AnalysisResult object
        """
        try:
            # Prepare prompt
            content_preview = article.content[:2000]  # Limit content length
            user_prompt = self.ANALYSIS_PROMPT_TEMPLATE.format(
                title=article.title, source=article.source, content=content_preview
            )

            # Call LLM
            messages = [
                SystemMessage(content=self.SYSTEM_PROMPT),
                HumanMessage(content=user_prompt),
            ]

            response = await self.llm.ainvoke(messages)

            # Parse response
            analysis_data = self._parse_response(response.content)

            # Create AnalysisResult
            return AnalysisResult(
                article=article,
                title_cn=analysis_data["title_cn"][:200],  # Enforce max length
                summary=analysis_data["summary"][:150],  # Enforce max length
                category=analysis_data["category"],
                importance_score=min(10, max(0, analysis_data["importance_score"])),
                insight=analysis_data["insight"][:100],  # Enforce max length
            )

        except Exception as e:
            print(f"Error analyzing article '{article.title}': {e}")
            # Return default analysis on error
            return AnalysisResult(
                article=article,
                title_cn=article.title[:200],  # Fallback to original title
                summary=article.title[:150],
                category="Tools/Products",
                importance_score=5,
                insight="åˆ†æžå¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼",
            )

    def _parse_response(self, response_text: str) -> dict:
        """Parse LLM response to extract analysis data.

        Args:
            response_text: LLM response text

        Returns:
            Dict with analysis fields
        """
        try:
            # Try to extract JSON from response
            # Handle potential markdown code blocks
            text = response_text.strip()

            # Remove markdown code blocks if present
            if text.startswith("```"):
                text = text.split("```")[1]
                if text.startswith("json"):
                    text = text[4:]
                text = text.strip()

            # Parse JSON
            data = json.loads(text)

            # Validate required fields
            required_fields = ["title_cn", "summary", "category", "importance_score", "insight"]
            for field in required_fields:
                if field not in data:
                    raise ValueError(f"Missing required field: {field}")

            # Validate category
            valid_categories: List[CategoryType] = [
                "Breaking News",
                "Research",
                "Tools/Products",
                "Business",
                "Tutorial",
            ]
            if data["category"] not in valid_categories:
                data["category"] = "Tools/Products"

            return data

        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            print(f"Response text: {response_text}")
            raise
